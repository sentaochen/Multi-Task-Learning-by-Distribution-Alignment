{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import autograd as ag\n",
    "import autograd.numpy as anp\n",
    "import scipy.optimize as opt\n",
    "from sklearn.metrics.pairwise import rbf_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please make sure that the package autograd is installed. If not, install it via the command !pip install autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance 1\n",
    "\n",
    "class MTL_H_Q:\n",
    "    \"\"\" \n",
    "    Multi-Task Learning by Distribution Alignment (hinge loss, quadratic seed function)\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma: float, default: 1.0\n",
    "        Kernel bandwidth of the rbf kernel.\n",
    "    lamda: float, default: 1.0\n",
    "        Tradeoff parameter.\n",
    "    gamma: float, default: 1e-3\n",
    "        Regularization parameter.    \n",
    "        \n",
    "    \"\"\" \n",
    "    def __init__(self,sigma=1.0,lamda=1.0,gamma=1e-3):\n",
    "        args_values = locals()\n",
    "        args_values.pop(\"self\")\n",
    "        for arg,value in args_values.items():\n",
    "            setattr(self,arg,value) \n",
    "    \n",
    "    def fit(self, Xt, yt, X, y):\n",
    "        \"\"\"Fit the models according to the data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Xt : array, shape = [mt, d]\n",
    "             Data from task t, where mt is the number of samples\n",
    "             and d is the number of features.\n",
    "\n",
    "        yt : array-like, shape = [mt]\n",
    "             Class labels from task t.\n",
    "        \n",
    "        X : array, shape = [m, d]\n",
    "            Data from all tasks, where m is the number of samples\n",
    "            and d is the number of features.\n",
    "\n",
    "        y : array-like, shape = [m]\n",
    "            Class labels from all tasks.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self.\n",
    "        \"\"\"\n",
    "        \n",
    "        m = X.shape[0]  \n",
    "        \n",
    "        # construct the feature kernel matrix   \n",
    "        K,Kt = rbf_kernel(X,X,1./self.sigma),rbf_kernel(Xt,X,1./self.sigma)   \n",
    "            \n",
    "        # construct the label kernel matrix\n",
    "        D,Dt = 1.0 * (y[:,None]==y), 1.0 * (yt[:,None]==y)\n",
    "            \n",
    "        def obj(vec): \n",
    "            hinge_loss = anp.mean(anp.exp(anp.dot(K * D,vec[m:])) * anp.maximum(0,1.0 - y * anp.dot(K,vec[:m])))\n",
    "            quadratic_loss = 0.5 * anp.mean(anp.exp(2 * anp.dot(K * D,vec[m:]))) - anp.mean(anp.exp(anp.dot(Kt * Dt,vec[m:])))\n",
    "            regularizer = anp.dot(vec[:m],vec[:m]) + anp.dot(vec[m:],vec[m:])                    \n",
    "            return hinge_loss + self.lamda * quadratic_loss + self.gamma * regularizer     \n",
    "            \n",
    "        res = opt.minimize(fun=obj,jac=ag.grad(obj), x0=1e-3 * np.random.rand(2 * m) ,method='L-BFGS-B')\n",
    "        \n",
    "        self.alpha = res.x[:m]\n",
    "        self.X = X\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict(self, Xt):\n",
    "        \"\"\" \n",
    "        Parameters\n",
    "        ----------\n",
    "        Xt : array, shape = [mt, d]\n",
    "             Data from task t, where mt is the number of samples\n",
    "             and d is the number of features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : array, shape = [mt]\n",
    "                 Predicted class labels for data in Xt.\n",
    "        \"\"\"\n",
    "        \n",
    "        # construct the feature kernel matrix       \n",
    "        Kt = rbf_kernel(Xt,self.X,1./self.sigma)   \n",
    "\n",
    "        predictions = np.where(Kt.dot(self.alpha) < 0,-1,1)\n",
    "        return predictions\n",
    "    \n",
    "    def score(self,Xt,yt):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        Xt : array, shape = [mt, d]\n",
    "        \n",
    "        yt : array-like, shape = [mt]\n",
    "        Returns\n",
    "        -------\n",
    "        test score : scalar\n",
    "        \"\"\" \n",
    "        y_hat = self.predict(Xt)\n",
    "        acc = np.mean(yt==y_hat)\n",
    "        return acc  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance 2\n",
    "\n",
    "class MTL_E_L:\n",
    "    \"\"\" \n",
    "    Multi-Task Learning by Distribution Alignment (exponential loss, log seed function)\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma: float, default: 1.0\n",
    "        Kernel bandwidth of the rbf kernel.\n",
    "    lamda: float, default: 1.0\n",
    "        Tradeoff parameter.\n",
    "    gamma: float, default: 1e-3\n",
    "        Regularization parameter.    \n",
    "        \n",
    "    \"\"\" \n",
    "    def __init__(self,sigma=1.0,lamda=1.0,gamma=1e-3):\n",
    "        args_values = locals()\n",
    "        args_values.pop(\"self\")\n",
    "        for arg,value in args_values.items():\n",
    "            setattr(self,arg,value) \n",
    "    \n",
    "    def fit(self, Xt, yt, X, y):\n",
    "        \"\"\"Fit the models according to the data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Xt : array, shape = [mt, d]\n",
    "             Data from task t, where mt is the number of samples\n",
    "             and d is the number of features.\n",
    "\n",
    "        yt : array-like, shape = [mt]\n",
    "             Class labels from task t.\n",
    "        \n",
    "        X : array, shape = [m, d]\n",
    "            Data from all tasks, where m is the number of samples\n",
    "            and d is the number of features.\n",
    "\n",
    "        y : array-like, shape = [m]\n",
    "            Class labels from all tasks.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self.\n",
    "        \"\"\"\n",
    "        \n",
    "        m = X.shape[0]  \n",
    "        \n",
    "        # construct the feature kernel matrix   \n",
    "        K,Kt = rbf_kernel(X,X,1./self.sigma),rbf_kernel(Xt,X,1./self.sigma)   \n",
    "            \n",
    "        # construct the label kernel matrix\n",
    "        D,Dt = 1.0 * (y[:,None]==y), 1.0 * (yt[:,None]==y)\n",
    "            \n",
    "        def obj(vec): \n",
    "            hinge_loss = anp.mean(anp.exp(anp.dot(K * D,vec[m:])) * anp.exp(- y * anp.dot(K,vec[:m])))\n",
    "            quadratic_loss = anp.mean(anp.exp(anp.dot(K * D,vec[m:]))) - anp.mean(anp.dot(Kt * Dt,vec[m:]))\n",
    "            regularizer = anp.dot(vec[:m],vec[:m]) + anp.dot(vec[m:],vec[m:])                    \n",
    "            return hinge_loss + self.lamda * quadratic_loss + self.gamma * regularizer     \n",
    "            \n",
    "        res = opt.minimize(fun=obj,jac=ag.grad(obj), x0=1e-3 * np.random.rand(2 * m) ,method='L-BFGS-B')\n",
    "        \n",
    "        self.alpha = res.x[:m]\n",
    "        self.X = X\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict(self, Xt):\n",
    "        \"\"\" \n",
    "        Parameters\n",
    "        ----------\n",
    "        Xt : array, shape = [mt, d]\n",
    "             Data from task t, where mt is the number of samples\n",
    "             and d is the number of features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : array, shape = [mt]\n",
    "                 Predicted class labels for data in Xt.\n",
    "        \"\"\"\n",
    "        \n",
    "        # construct the feature kernel matrix       \n",
    "        Kt = rbf_kernel(Xt,self.X,1./self.sigma)   \n",
    "\n",
    "        predictions = np.where(Kt.dot(self.alpha) < 0,-1,1)\n",
    "        return predictions\n",
    "    \n",
    "    def score(self,Xt,yt):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        Xt : array, shape = [mt, d]\n",
    "        \n",
    "        yt : array-like, shape = [mt]\n",
    "        Returns\n",
    "        -------\n",
    "        test score : scalar\n",
    "        \"\"\" \n",
    "        y_hat = self.predict(Xt)\n",
    "        acc = np.mean(yt==y_hat)\n",
    "        return acc  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import numpy.linalg as la\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(task,alltasks):\n",
    "    scaler = StandardScaler()\n",
    "    prefix = 'C:/Users/sentao/NoteBook3/8th work/'\n",
    "    \n",
    "    data = sio.loadmat(prefix + 'datasets/PIE/' + task + '.mat')\n",
    "    Xt,yt = data['fea'].astype(np.float64),data['gnd'].ravel().astype(np.float64)\n",
    "    Xt,yt = np.vstack((Xt[yt<=30],Xt[yt>30])),np.hstack((yt[yt<=30],yt[yt>30]))\n",
    "    yt[yt<=30],yt[yt>30] = -1,1\n",
    "    Xt, Xtest, yt, ytest = train_test_split(Xt, yt, test_size=0.8,random_state=0)\n",
    "    \n",
    "    X_list,y_list = [Xt],[yt]\n",
    "    for t in alltasks:\n",
    "        if t!=task:\n",
    "            data = sio.loadmat(prefix + 'datasets/PIE/' + t + '.mat')\n",
    "            Xt,yt = data['fea'].astype(np.float64),data['gnd'].ravel().astype(np.float64)\n",
    "            Xt,yt = np.vstack((Xt[yt<=30],Xt[yt>30])),np.hstack((yt[yt<=30],yt[yt>30]))\n",
    "            yt[yt<=30],yt[yt>30] = -1,1 \n",
    "            X_list.append(Xt),y_list.append(yt)\n",
    "            \n",
    "    X, y = np.concatenate(X_list),np.concatenate(y_list)      \n",
    "    scaler.fit(X)\n",
    "    Xt, Xtest, X = scaler.transform(Xt),scaler.transform(Xtest),scaler.transform(X)\n",
    "    \n",
    "    return Xt,yt,X,y,Xtest,ytest\n",
    "\n",
    "alltasks = ['PIE05','PIE27','PIE29']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIE05 svm: 0.8484621155288822 ours: 0.8432108027006752\n",
      "PIE27 svm: 0.8667417417417418 ours: 0.8412162162162162\n",
      "PIE29 svm: 0.8093415007656968 ours: 0.7825421133231241\n"
     ]
    }
   ],
   "source": [
    "for task in alltasks:\n",
    "    Xt,yt,X,y,Xtest,ytest = readData(task,alltasks)      \n",
    "    clf = MTL_H_Q(sigma=0.5,lamda=1,gamma=1e-2)\n",
    "    print(task,'svm:', SVC(kernel='linear').fit(X,y).score(Xtest,ytest),'ours:',clf.fit(Xt,yt,X,y).score(Xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIE05 svm: 0.8484621155288822 ours: 0.8570892723180795\n",
      "PIE27 svm: 0.8667417417417418 ours: 0.850975975975976\n",
      "PIE29 svm: 0.8093415007656968 ours: 0.781010719754977\n"
     ]
    }
   ],
   "source": [
    "for task in alltasks:\n",
    "    Xt,yt,X,y,Xtest,ytest = readData(task,alltasks)      \n",
    "    clf = MTL_H_Q(sigma=5,lamda=1,gamma=1e-2)\n",
    "    print(task,'svm:', SVC(kernel='linear').fit(X,y).score(Xtest,ytest),'ours:',clf.fit(Xt,yt,X,y).score(Xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in alltasks:\n",
    "    Xt,yt,X,y,Xtest,ytest = readData(task,alltasks)      \n",
    "    clf = MTL_E_L(sigma=0.5,lamda=1,gamma=1e-2)\n",
    "    print(task,'svm:', SVC(kernel='linear').fit(X,y).score(Xtest,ytest),'ours:',clf.fit(Xt,yt,X,y).score(Xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in alltasks:\n",
    "    Xt,yt,X,y,Xtest,ytest = readData(task,alltasks)      \n",
    "    clf = MTL_E_L(sigma=5,lamda=1,gamma=1e-2)\n",
    "    print(task,'svm:', SVC(kernel='linear').fit(X,y).score(Xtest,ytest),'ours:',clf.fit(Xt,yt,X,y).score(Xtest,ytest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
